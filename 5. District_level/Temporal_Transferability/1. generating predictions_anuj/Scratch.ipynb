{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on intermediate years using best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7401195411888216, 0.7248345257490144, 0.7105458596357251, 0.7043082789836379, 0.6973348478544594]\n",
      "-------------\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "emp_femtss = pickle.load(open('xgBoost_models/emp_femaletop_score_specs.pkl','rb'))\n",
    "print(emp_femtss['xgBoost']['val_scores'])\n",
    "print('-------------')\n",
    "print(emp_femtss['xgBoost']['train_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesing on MSL-XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "def apply_smote(data, feature_cols, target):\n",
    "    '''\n",
    "    Input:\n",
    "    data - the original dataframe\n",
    "    feature_cols - the feature columns (list of columns)\n",
    "    target - the target column (string value)\n",
    "    '''\n",
    "    sm = SMOTE(random_state=42)\n",
    "    features, targets = sm.fit_resample(data[feature_cols],data[target])\n",
    "    feature_df = pd.DataFrame(features, columns=feature_cols)\n",
    "    target_df = pd.DataFrame(targets, columns=[target])\n",
    "    output = pd.concat([feature_df, target_df], axis=1)\n",
    "    # Shuffling dataset\n",
    "    output = output.sample(frac=1).reset_index(drop=True)\n",
    "    return output\n",
    "filepath = '2011_districts_10_bins_v2.csv'\n",
    "data = pd.read_csv(filepath)\n",
    "feature_cols = data.columns[1:-10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['censuscode', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
       "       ...\n",
       "       'MSW+AF8-2011', 'BF+AF8-2011', 'MSL+AF8-2011', 'FC+AF8-2011',\n",
       "       'CHH+AF8-2011', 'EMP+AF8-2011', 'ASSET+AF8-2011', 'EMP_FEMALE+AF8-2011',\n",
       "       'LIT+AF8-2011', 'EMP_AG_NONAG+AF8-2011'],\n",
       "      dtype='object', length=131)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    243\n",
      "1    243\n",
      "0    243\n",
      "Name: EMP_FEMALE+AF8-2011, dtype: int64\n",
      "(729, 120)\n",
      "(729,)\n"
     ]
    }
   ],
   "source": [
    "data['EMP_FEMALE+AF8-2011'] = data['EMP_FEMALE+AF8-2011'] -1\n",
    "y_col_EMP_FEMALE = 'EMP_FEMALE+AF8-2011'\n",
    "output = apply_smote(data, feature_cols, y_col_EMP_FEMALE)\n",
    "print(output['EMP_FEMALE+AF8-2011'].value_counts())\n",
    "X = output[feature_cols].values\n",
    "y = output[y_col_EMP_FEMALE].values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'max_depth': 10,\n",
       " 'learning_rate': 0.05,\n",
       " 'verbosity': 1,\n",
       " 'objective': 'binary:logistic',\n",
       " 'booster': 'dart',\n",
       " 'n_jobs': -1,\n",
       " 'gamma': 0,\n",
       " 'min_child_weight': 0,\n",
       " 'max_delta_step': 0,\n",
       " 'subsample': 1,\n",
       " 'colsample_bytree': 1,\n",
       " 'colsample_bylevel': 1,\n",
       " 'colsample_bynode': 0.7,\n",
       " 'reg_alpha': 0,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'base_score': 0.5,\n",
       " 'random_state': 0,\n",
       " 'importance_type': 'weight',\n",
       " 'n_randomized_search': 200,\n",
       " 'run_type': 'train-test',\n",
       " 'trail': 0,\n",
       " 'trail_type': 'sales',\n",
       " 'train_ratio': 0.9,\n",
       " 'val_train_loss': 'f1_weighted',\n",
       " 'top_n_models_drill': 5,\n",
       " 'top_n_models_pred': 5,\n",
       " 'n_drill_search': [50],\n",
       " 'test_ratio': 0.0,\n",
       " 'y_col': 'EMP_FEMALE+AF8-2011',\n",
       " 'cols_to_drop': [],\n",
       " 'ensemble': False,\n",
       " 'n_cols_dropped': 0,\n",
       " 'current_date_col': 'random_drop',\n",
       " 'no_steps_ahead': 0,\n",
       " 'time_cols': [],\n",
       " 'cross_validation': ['KFold'],\n",
       " 'kFold_splits': 5,\n",
       " 'pFold_splits': 5,\n",
       " 'feature_cols': ['1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  '5',\n",
       "  '6',\n",
       "  '7',\n",
       "  '8',\n",
       "  '9',\n",
       "  '10',\n",
       "  '11',\n",
       "  '12',\n",
       "  '13',\n",
       "  '14',\n",
       "  '15',\n",
       "  '16',\n",
       "  '17',\n",
       "  '18',\n",
       "  '19',\n",
       "  '20',\n",
       "  '21',\n",
       "  '22',\n",
       "  '23',\n",
       "  '24',\n",
       "  '25',\n",
       "  '26',\n",
       "  '27',\n",
       "  '28',\n",
       "  '29',\n",
       "  '30',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '35',\n",
       "  '36',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '40',\n",
       "  '41',\n",
       "  '42',\n",
       "  '43',\n",
       "  '44',\n",
       "  '45',\n",
       "  '46',\n",
       "  '47',\n",
       "  '48',\n",
       "  '49',\n",
       "  '50',\n",
       "  '51',\n",
       "  '52',\n",
       "  '53',\n",
       "  '54',\n",
       "  '55',\n",
       "  '56',\n",
       "  '57',\n",
       "  '58',\n",
       "  '59',\n",
       "  '60',\n",
       "  '61',\n",
       "  '62',\n",
       "  '63',\n",
       "  '64',\n",
       "  '65',\n",
       "  '66',\n",
       "  '67',\n",
       "  '68',\n",
       "  '69',\n",
       "  '70',\n",
       "  '71',\n",
       "  '72',\n",
       "  '73',\n",
       "  '74',\n",
       "  '75',\n",
       "  '76',\n",
       "  '77',\n",
       "  '78',\n",
       "  '79',\n",
       "  '80',\n",
       "  '81',\n",
       "  '82',\n",
       "  '83',\n",
       "  '84',\n",
       "  '85',\n",
       "  '86',\n",
       "  '87',\n",
       "  '88',\n",
       "  '89',\n",
       "  '90',\n",
       "  '91',\n",
       "  '92',\n",
       "  '93',\n",
       "  '94',\n",
       "  '95',\n",
       "  '96',\n",
       "  '97',\n",
       "  '98',\n",
       "  '99',\n",
       "  '100',\n",
       "  '101',\n",
       "  '102',\n",
       "  '103',\n",
       "  '104',\n",
       "  '105',\n",
       "  '106',\n",
       "  '107',\n",
       "  '108',\n",
       "  '109',\n",
       "  '110',\n",
       "  '111',\n",
       "  '112',\n",
       "  '113',\n",
       "  '114',\n",
       "  '115',\n",
       "  '116',\n",
       "  '117',\n",
       "  '118',\n",
       "  '119',\n",
       "  '120'],\n",
       " 'input_order': Index(['1', '2', '3', '4', '5', '6', '7', '8', '9', '10',\n",
       "        ...\n",
       "        '112', '113', '114', '115', '116', '117', '118', '119', '120',\n",
       "        'EMP_FEMALE+AF8-2011'],\n",
       "       dtype='object', length=121)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_femtss['xgBoost']['specs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = emp_femtss['xgBoost']['specs'][0]['n_estimators']\n",
    "max_depth = emp_femtss['xgBoost']['specs'][0]['max_depth']\n",
    "learning_rate = emp_femtss['xgBoost']['specs'][0]['learning_rate']\n",
    "objective = emp_femtss['xgBoost']['specs'][0]['objective']\n",
    "booster = emp_femtss['xgBoost']['specs'][0]['booster']\n",
    "gamma = emp_femtss['xgBoost']['specs'][0]['gamma']\n",
    "min_child_weight = emp_femtss['xgBoost']['specs'][0]['min_child_weight']\n",
    "max_delta_step = emp_femtss['xgBoost']['specs'][0]['max_delta_step']\n",
    "subsample = emp_femtss['xgBoost']['specs'][0]['subsample']\n",
    "colsample_bytree = emp_femtss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "colsample_bylevel = emp_femtss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "colsample_bynode = emp_femtss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "reg_alpha = emp_femtss['xgBoost']['specs'][0]['reg_alpha']\n",
    "reg_lambda = emp_femtss['xgBoost']['specs'][0]['reg_lambda']\n",
    "scale_pos_weight = emp_femtss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "base_score = emp_femtss['xgBoost']['specs'][0]['base_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "                     max_depth=max_depth, learning_rate=learning_rate, \n",
    "                     objective=objective, booster=booster,n_jobs=-1, \n",
    "                     gamma=gamma, min_child_weight=min_child_weight, \n",
    "                     max_delta_step=max_delta_step, subsample=subsample, \n",
    "                     colsample_bytree=colsample_bytree, \n",
    "                     colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "                     reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                     scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "cv = KFold(n_splits=n_splits, shuffle=True)\n",
    "val_score = []\n",
    "train_score = []\n",
    "for train_index, test_index in cv.split(X):\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    best_xgb = xgbc.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = best_xgb.predict(X_test)\n",
    "    predictions_train = best_xgb.predict(X_train)\n",
    "    f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "    f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "    \n",
    "    val_score.append(f1_weight)\n",
    "    train_score.append(f1_weight_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7407195661869101\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "val_score = np.array(val_score)\n",
    "print(val_score.mean())\n",
    "train_score = np.array(train_score)\n",
    "print(train_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n",
       "       colsample_bynode=0.7, colsample_bytree=1, gamma=0,\n",
       "       learning_rate=0.05, max_delta_step=0, max_depth=10,\n",
       "       min_child_weight=0, missing=None, n_estimators=1000, n_jobs=-1,\n",
       "       nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_xgb, open('best_xgb_emp_fem.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = pickle.load(open('best_xgb_emp_fem.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r\"C:\\Users\\AJain7\\OneDrive - Stryker\\Personal\\Projects\\Satellite Project\\5. District_level\\Change Classifier\\Archive_Anuj-Anupam\\quantile_csv_past\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for files in os.listdir(folder):\n",
    "    filepath = os.path.join(folder, files)\n",
    "    year = files[:4]\n",
    "    df = pd.read_csv(filepath)\n",
    "    feature_cols = df.columns[1:]\n",
    "    df = df[feature_cols]\n",
    "#     print(df)\n",
    "    for index, row in df.iterrows():\n",
    "        X_test = row[1:]\n",
    "        prediction = best_xgb.predict(X_test)\n",
    "        output_list.append([year,prediction[0], row[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2001', 2, 57]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_list[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data=output_list,columns=['year', 'predictions', 'district_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('predictions_past_data_emp_fem.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filee = r\"C:\\Users\\AJain7\\OneDrive - Stryker\\Personal\\Projects\\Satellite Project\\5. District_level\\Change Classifier\\Archive_Anuj-Anupam\\quantile_csv\\2001_districts_quant.csv\"\n",
    "\n",
    "# ff = pd.read_csv(filee)\n",
    "\n",
    "# feature_cols = ff.columns[1:]\n",
    "\n",
    "# ff = ff[feature_cols]\n",
    "\n",
    "# output = pd.DataFrame(data=output_list,columns=['year', 'district_code', 'predictions'])\n",
    "\n",
    "# output_list = []\n",
    "# for index, row in ff.iterrows():\n",
    "#     X_test = row[1:]\n",
    "#     prediction = best_xgb.predict(X_test)\n",
    "# #     print(prediction[0], row[0], '2011')\n",
    "#     output_list.append(['2011',prediction[0], row[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
