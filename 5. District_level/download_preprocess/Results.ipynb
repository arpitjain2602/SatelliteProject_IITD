{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains - \n",
    "\n",
    "1. Calculating normal f1_scores for 2001 and 2011\n",
    "2. Calculating cross-year f1_scores for 2001 and 2011\n",
    "3. Creating data for change classifier that includes doing the prediciton between 2001-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "folder = '/Users/arpitjain/Desktop/all_results/all_results'\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSET_2001AnujMethod_RF_top_score_specs@8.pkl\n",
      "FC_2011AnujMethod2011_XGB_top_score_specs@10.pkl\n",
      "CHH_2001AnujMethod2001_XGB_top_score_specs@9.pkl\n",
      "CHH_2001AnujMethod2001_XGB_top_score_specs@8.pkl\n",
      "FC_2011AnujMethod2011_XGB_top_score_specs@7.pkl\n",
      "MSL_2011AnujMethod2011_XGB_top_score_specs@7.pkl\n",
      "MSW_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "FC_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "ASSET_2001AnujMethod2001_XGB_top_score_specs@8.pkl\n",
      "ASSET_2001AnujMethod2001_XGB_top_score_specs@9.pkl\n",
      "MSL_2001AnujMethod_RF_top_score_specs@8.pkl\n",
      "MSW_2011AnujMethod2011_XGB_top_score_specs@10.pkl\n",
      "ASSET_2011AnujMethod2011_XGB_top_score_specs@7.pkl\n",
      "MSL_2001AnujMethod2001_XGB_top_score_specs@10.pkl\n",
      "MSW_2001AnujMethod2001_XGB_top_score_specs@6.pkl\n",
      "MSL_2001AnujMethod2001_XGB_top_score_specs@11.pkl\n",
      "MSL_2001AnujMethod2001_XGB_top_score_specs@9.pkl\n",
      "MSW_2001AnujMethod2001_XGB_top_score_specs@5.pkl\n",
      "BF_2001AnujMethod2001_XGB_top_score_specs@10.pkl\n",
      "ASSET_2001AnujMethod2001_XGB_top_score_specs@10.pkl\n",
      "BF_2001AnujMethod_RF_top_score_specs@6.pkl\n",
      "ASSET_2001AnujMethod2001_XGB_top_score_specs@11.pkl\n",
      "BF_2001AnujMethod2001_XGB_top_score_specs@11.pkl\n",
      "FC_2001AnujMethod2001_XGB_top_score_specs@8.pkl\n",
      "MSL_2001AnujMethod2001_XGB_top_score_specs@8.pkl\n",
      "CHH_2011AnujMethod2011_XGB_top_score_specs@7.pkl\n",
      "MSW_2001AnujMethod_RF_top_score_specs@8.pkl\n",
      "FC_2001AnujMethod_RF_top_score_specs@6.pkl\n",
      "FC_2001AnujMethod2001_XGB_top_score_specs@11.pkl\n",
      "CHH_2001AnujMethod_RF_top_score_specs@6.pkl\n",
      "FC_2001AnujMethod2001_XGB_top_score_specs@10.pkl\n",
      "MSW_2001AnujMethod2001_XGB_top_score_specs@10.pkl\n",
      "MSW_2001AnujMethod2001_XGB_top_score_specs@11.pkl\n",
      "BF_2001AnujMethod2001_XGB_top_score_specs@6.pkl\n",
      "MSL_2001AnujMethod_RF_top_score_specs@10.pkl\n",
      "MSL_2011AnujMethod2011_XGB_top_score_specs@10.pkl\n",
      "BF_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "ASSET_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "BF_2001AnujMethod2001_XGB_top_score_specs@5.pkl\n",
      "MSL_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "MSL_2001AnujMethod_RF_top_score_specs@13.pkl\n",
      "BF_2011AnujMethod2011_XGB_top_score_specs@10.pkl\n",
      "BF_2001AnujMethod2001_XGB_top_score_specs@9.pkl\n",
      "BF_2001AnujMethod2001_XGB_top_score_specs@8.pkl\n",
      "CHH_2001AnujMethod_RF_top_score_specs@10.pkl\n",
      "CHH_2001AnujMethod2001_XGB_top_score_specs@11.pkl\n",
      "CHH_2001AnujMethod2001_XGB_top_score_specs@10.pkl\n",
      "CHH_2001AnujMethod_RF_top_score_specs@8.pkl\n",
      "FC_2001AnujMethod_RF_top_score_specs@8.pkl\n",
      "MSW_2001AnujMethod_RF_top_score_specs@6.pkl\n",
      "BF_2011AnujMethod2011_XGB_top_score_specs@7.pkl\n",
      "FC_2001AnujMethod2001_XGB_top_score_specs@5.pkl\n",
      "MSL_2001AnujMethod2001_XGB_top_score_specs@5.pkl\n",
      "BF_2001AnujMethod_RF_top_score_specs@13.pkl\n",
      "MSW_2001AnujMethod2001_XGB_top_score_specs@9.pkl\n",
      "ASSET_2001AnujMethod_RF_top_score_specs@10.pkl\n",
      "FC_2001AnujMethod_RF_top_score_specs@10.pkl\n",
      "MSW_2001AnujMethod2001_XGB_top_score_specs@8.pkl\n",
      "MSL_2001AnujMethod2001_XGB_top_score_specs@6.pkl\n",
      "FC_2001AnujMethod2001_XGB_top_score_specs@6.pkl\n",
      "BF_2001AnujMethod_RF_top_score_specs@10.pkl\n",
      "BF_2001AnujMethod_RF_top_score_specs@8.pkl\n",
      "FC_2001AnujMethod_RF_top_score_specs@13.pkl\n",
      "CHH_2001AnujMethod2001_XGB_top_score_specs@6.pkl\n",
      "ASSET_2001AnujMethod2001_XGB_top_score_specs@5.pkl\n",
      "ASSET_2001AnujMethod_RF_top_score_specs@6.pkl\n",
      "CHH_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "MSL_2001AnujMethod_RF_top_score_specs@6.pkl\n",
      "MSW_2001AnujMethod_RF_top_score_specs@10.pkl\n",
      "CHH_2001AnujMethod2001_XGB_top_score_specs@5.pkl\n",
      "ASSET_2001AnujMethod2001_XGB_top_score_specs@6.pkl\n",
      "MSW_2011AnujMethod2011_XGB_top_score_specs@7.pkl\n"
     ]
    }
   ],
   "source": [
    "for files in os.listdir(folder):\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(data, feature_cols, target):\n",
    "    '''\n",
    "    Input:\n",
    "    data - the original dataframe\n",
    "    feature_cols - the feature columns (list of columns)\n",
    "    target - the target column (string value)\n",
    "    '''\n",
    "    sm = SMOTE(random_state=42)\n",
    "    features, targets = sm.fit_resample(data[feature_cols],data[target])\n",
    "    feature_df = pd.DataFrame(features, columns=feature_cols)\n",
    "    target_df = pd.DataFrame(targets, columns=[target])\n",
    "    output = pd.concat([feature_df, target_df], axis=1)\n",
    "    # Shuffling dataset\n",
    "    output = output.sample(frac=1).reset_index(drop=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_filepath(year, bincount):\n",
    "    a = '/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@5.csv'\n",
    "    filepath = a.replace('2001', str(year))\n",
    "    filepath = filepath.replace('@5', '@'+str(bincount))\n",
    "#     print(filepath)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "ground_truth = ground_truth[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/District - Ground Truth - 2011.csv\")\n",
    "cols = ['census_code','MSW_2011','BF_2011','MSL_2011', 'FC_2011','CHH_2011','ASSET_2011']\n",
    "ground_truth = ground_truth[cols]\n",
    "filepath = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2011_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@9.csv\"\n",
    "# df = pd.read_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@9.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "# For 2001\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "y_col = 'BF_2001'\n",
    "feature_cols = data.columns[7:]\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = apply_smote(data, feature_cols, y_col)\n",
    "X = output[feature_cols].values\n",
    "y = output[y_col].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "13\n",
      "13  #####  CHH_2011AnujMethod2011_XGB_top_score_specs@13.pkl\n",
      "[0.6431964330130807, 0.6376292182829371, 0.6361804097544307, 0.6302816947158044, 0.6171012521632712]\n",
      "[0.9974629038832928, 0.997474897684118, 0.9974769420885681, 0.9974684131183981, 0.9977780641771448]\n",
      "--------------------\n",
      "0.6357626977331742 0.9974491755031123 0.6357626977331742 0.9974491755031123\n"
     ]
    }
   ],
   "source": [
    "for files in os.listdir(folder):\n",
    "    if '2011' in files:\n",
    "        if 'CHH' in files:\n",
    "#             print(files)\n",
    "            if 'XGB' in files:\n",
    "                bincount = files.split(\"@\")[1].split(\".\")[0]\n",
    "                print(bincount)\n",
    "                if (bincount == '13'):\n",
    "                    print(bincount,' ##### ' ,files)\n",
    "                    tss = pickle.load(open(os.path.join(folder, files), \"rb\"))\n",
    "                    print(tss['xgBoost']['val_scores'])\n",
    "                    print(tss['xgBoost']['train_scores'])\n",
    "                    print('--------------------')\n",
    "\n",
    "#                     filepath = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@9.csv\"\n",
    "#                     df = pd.read_csv(filepath)\n",
    "#                     # For 2001\n",
    "#                     ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "#                     cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "#                     ground_truth = ground_truth[cols]\n",
    "#                     data = ground_truth.merge(df, how='left', on='census_code')\n",
    "#                     data.dropna(inplace=True)\n",
    "                    \n",
    "                    ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/District - Ground Truth - 2011.csv\")\n",
    "                    cols = ['census_code','MSW_2011','BF_2011','MSL_2011', 'FC_2011','CHH_2011','ASSET_2011']\n",
    "                    ground_truth = ground_truth[cols]\n",
    "                    filepath = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2011_Anuj_Method/Features_100m_quantile@13.csv\"\n",
    "                    df = pd.read_csv(filepath)\n",
    "\n",
    "                    data = ground_truth.merge(df, how='left', on='census_code')\n",
    "                    data.dropna(inplace=True)\n",
    "\n",
    "                    y_col = 'CHH_2011'\n",
    "                    feature_cols = data.columns[7:]\n",
    "                    X = data[feature_cols].values\n",
    "                    y = data[y_col].values\n",
    "\n",
    "                    n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "                    max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "                    learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "                    objective = tss['xgBoost']['specs'][0]['objective']\n",
    "                    booster = tss['xgBoost']['specs'][0]['booster']\n",
    "                    gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "                    min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "                    max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "                    subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "                    colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "                    colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "                    colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "                    reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "                    reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "                    scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "                    base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "                    n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "                    xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "                         max_depth=max_depth, learning_rate=learning_rate, \n",
    "                         objective=objective, booster=booster,n_jobs=-1, \n",
    "                         gamma=gamma, min_child_weight=min_child_weight, \n",
    "                         max_delta_step=max_delta_step, subsample=subsample, \n",
    "                         colsample_bytree=colsample_bytree, \n",
    "                         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "                         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "                    cv = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "\n",
    "                    val_f1score = []\n",
    "                    val_accscore = []\n",
    "                    train_f1score = []\n",
    "                    train_accscore = []\n",
    "                    for train_index, test_index in cv.split(X):\n",
    "                        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "\n",
    "                        sm = SMOTE(random_state=42)\n",
    "                        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "                        best_xgb = xgbc.fit(X_train, y_train)\n",
    "\n",
    "                        predictions = best_xgb.predict(X_test)\n",
    "                        predictions_train = best_xgb.predict(X_train)\n",
    "\n",
    "                        f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "                        f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "                        acc = accuracy_score(y_test, predictions)\n",
    "                        acc_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "                        val_f1score.append(f1_weight)\n",
    "                        train_f1score.append(f1_weight_train)\n",
    "                        val_accscore.append(f1_weight)\n",
    "                        train_accscore.append(f1_weight_train)\n",
    "\n",
    "                    val_f1score = np.array(val_f1score).mean()\n",
    "                    train_f1score = np.array(train_f1score).mean()\n",
    "                    val_accscore = np.array(val_accscore).mean()\n",
    "                    train_accscore = np.array(train_accscore).mean()\n",
    "\n",
    "                    print(val_accscore ,train_accscore, val_f1score, train_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in os.listdir(folder):\n",
    "    if '2011' in files:\n",
    "        print(files)\n",
    "        if 'CHH' in files:\n",
    "            if 'XGB' in files:\n",
    "                bincount = files.split(\"@\")[1].split(\".\")[0]\n",
    "                print(bincount)\n",
    "                if (bincount == '9'):\n",
    "                    print(bincount,' ##### ' ,files)\n",
    "                    tss = pickle.load(open(os.path.join(folder, files), \"rb\"))\n",
    "                    print(tss['xgBoost']['val_scores'])\n",
    "                    print(tss['xgBoost']['train_scores'])\n",
    "                    print('--------------------')\n",
    "\n",
    "                    filepath = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@9.csv\"\n",
    "                    df = pd.read_csv(filepath)\n",
    "                    # For 2001\n",
    "                    ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "                    cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "                    ground_truth = ground_truth[cols]\n",
    "                    data = ground_truth.merge(df, how='left', on='census_code')\n",
    "                    data.dropna(inplace=True)\n",
    "\n",
    "                    y_col = 'CHH_2011'\n",
    "                    feature_cols = data.columns[7:]\n",
    "                    X = data[feature_cols].values\n",
    "                    y = data[y_col].values\n",
    "\n",
    "                    n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "                    max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "                    learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "                    objective = tss['xgBoost']['specs'][0]['objective']\n",
    "                    booster = tss['xgBoost']['specs'][0]['booster']\n",
    "                    gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "                    min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "                    max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "                    subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "                    colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "                    colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "                    colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "                    reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "                    reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "                    scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "                    base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "                    n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "                    xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "                         max_depth=max_depth, learning_rate=learning_rate, \n",
    "                         objective=objective, booster=booster,n_jobs=-1, \n",
    "                         gamma=gamma, min_child_weight=min_child_weight, \n",
    "                         max_delta_step=max_delta_step, subsample=subsample, \n",
    "                         colsample_bytree=colsample_bytree, \n",
    "                         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "                         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "                    cv = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "\n",
    "                    val_f1score = []\n",
    "                    val_accscore = []\n",
    "                    train_f1score = []\n",
    "                    train_accscore = []\n",
    "                    for train_index, test_index in cv.split(X):\n",
    "                        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "\n",
    "                        sm = SMOTE(random_state=42)\n",
    "                        X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "                        best_xgb = xgbc.fit(X_train, y_train)\n",
    "\n",
    "                        predictions = best_xgb.predict(X_test)\n",
    "                        predictions_train = best_xgb.predict(X_train)\n",
    "\n",
    "                        f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "                        f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "                        acc = accuracy_score(y_test, predictions)\n",
    "                        acc_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "                        val_f1score.append(f1_weight)\n",
    "                        train_f1score.append(f1_weight_train)\n",
    "                        val_accscore.append(f1_weight)\n",
    "                        train_accscore.append(f1_weight_train)\n",
    "\n",
    "                    val_f1score = np.array(val_f1score).mean()\n",
    "                    train_f1score = np.array(train_f1score).mean()\n",
    "                    val_accscore = np.array(val_accscore).mean()\n",
    "                    train_accscore = np.array(train_accscore).mean()\n",
    "\n",
    "                    print(val_accscore ,train_accscore, val_f1score, train_f1score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = pickle.load(open(\"/Users/arpitjain/Desktop/all_results/all_results/BF_2011AnujMethod2011_XGB_top_score_specs@13.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['xgBoost'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tss.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_f1_score  [0.6896101764369367, 0.6763030618465768, 0.6733068700682265, 0.6720860524996761, 0.6589215637928102]\n",
      "train_f1_score  [0.9969294458349223, 0.9969413412944604, 0.9971919387580958, 0.997187138701654, 0.9971892187283403]\n"
     ]
    }
   ],
   "source": [
    "tss = pickle.load( open(\"/Users/arpitjain/Desktop/all_results/all_results/BF_2011AnujMethod2011_XGB_top_score_specs@13.pkl\", 'rb'))\n",
    "print('val_f1_score ',tss['xgBoost']['val_scores'])\n",
    "print('train_f1_score ',tss['xgBoost']['train_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    }
   ],
   "source": [
    "n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "objective = tss['xgBoost']['specs'][0]['objective']\n",
    "booster = tss['xgBoost']['specs'][0]['booster']\n",
    "gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "print('                 ')\n",
    "print('Re-Training again for calculating f1 and train scores')\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "         max_depth=max_depth, learning_rate=learning_rate, \n",
    "         objective=objective, booster=booster,n_jobs=-1, \n",
    "         gamma=gamma, min_child_weight=min_child_weight, \n",
    "         max_delta_step=max_delta_step, subsample=subsample, \n",
    "         colsample_bytree=colsample_bytree, \n",
    "         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587, 103)\n",
      "(587, 96) (587,)\n"
     ]
    }
   ],
   "source": [
    "# 2001\n",
    "df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2001_Anuj_Method/Features_100m_quantile@8.csv\")\n",
    "# For 2001\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "feature_cols = data.columns[7:].tolist()\n",
    "y_col = 'FC_2001'\n",
    "# output = apply_smote(data, feature_cols, y_col)\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633, 163)\n",
      "(633, 156) (633,)\n"
     ]
    }
   ],
   "source": [
    "# 2011\n",
    "df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2011_Anuj_Method/Features_100m_quantile@13.csv\")\n",
    "# For 2011\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/District - Ground Truth - 2011.csv\")\n",
    "cols = ['census_code','MSW_2011','BF_2011','MSL_2011', 'FC_2011','CHH_2011','ASSET_2011']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "feature_cols = data.columns[7:].tolist()\n",
    "y_col = 'BF_2011'\n",
    "# output = apply_smote(data, feature_cols, y_col)\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6491478481156241 0.9963279605929272 0.6614173228346457 0.9963235294117647\n",
      "1 0.7227221597300337 0.9948116183410302 0.7165354330708661 0.9948119325551232\n",
      "2 0.6327099737532809 0.9974747112418394 0.6377952755905512 0.9974747474747475\n",
      "3 0.6948868284183868 0.9960369016403634 0.6984126984126984 0.996031746031746\n",
      "4 0.6628411005053341 0.9961438792137759 0.6666666666666666 0.9961389961389961\n",
      "------\n",
      "0.6724615821045319 0.9961590142059871 0.6761654793150856 0.9961561903224755\n"
     ]
    }
   ],
   "source": [
    "val_f1score = []\n",
    "val_accscore = []\n",
    "train_f1score = []\n",
    "train_accscore = []\n",
    "# f1score = []\n",
    "# accscore = []\n",
    "counter=0\n",
    "for train_index, test_index in cv.split(X):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    \n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    \n",
    "    best_xgb = xgbc.fit(X_train, y_train)\n",
    "    \n",
    "#     prediction_2011 = best_xgb.predict(X_2011)\n",
    "#     f1_weight = f1_score(y_2011, prediction_2011, average='weighted')\n",
    "#     acc = accuracy_score(y_2011, prediction_2011)\n",
    "\n",
    "#     f1score.append(f1_weight)\n",
    "#     accscore.append(acc)\n",
    "\n",
    "    predictions = best_xgb.predict(X_test)\n",
    "    predictions_train = best_xgb.predict(X_train)\n",
    "\n",
    "    f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "    f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    acc_train = accuracy_score(y_train, predictions_train)\n",
    "    print(counter, f1_weight, f1_weight_train, acc, acc_train)\n",
    "    counter=counter+1\n",
    "\n",
    "    val_f1score.append(f1_weight)\n",
    "    train_f1score.append(f1_weight_train)\n",
    "    val_accscore.append(acc)\n",
    "    train_accscore.append(acc_train)\n",
    "\n",
    "val_f1score = np.array(val_f1score).mean()\n",
    "train_f1score = np.array(train_f1score).mean()\n",
    "val_accscore = np.array(val_accscore).mean()\n",
    "train_accscore = np.array(train_accscore).mean()\n",
    "print('------')\n",
    "print(val_f1score, train_f1score, val_accscore, train_accscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2001 trained 2011 prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_smote(data, feature_cols, target):\n",
    "#     '''\n",
    "#     Input:\n",
    "#     data - the original dataframe\n",
    "#     feature_cols - the feature columns (list of columns)\n",
    "#     target - the target column (string value)\n",
    "#     '''\n",
    "#     sm = SMOTE(random_state=42)\n",
    "#     features, targets = sm.fit_resample(data[feature_cols],data[target])\n",
    "#     feature_df = pd.DataFrame(features, columns=feature_cols)\n",
    "#     target_df = pd.DataFrame(targets, columns=[target])\n",
    "#     output = pd.concat([feature_df, target_df], axis=1)\n",
    "#     # Shuffling dataset\n",
    "#     output = output.sample(frac=1).reset_index(drop=True)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_f1_score  [0.6246681482218321, 0.6010042567658707, 0.5976318830651421, 0.5954377173907119, 0.5828834873359391]\n",
      "train_f1_score  [1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "tss = pickle.load( open(\"/Users/arpitjain/Desktop/all_results/all_results/CHH_2001AnujMethod2001_XGB_top_score_specs@9.pkl\", 'rb'))\n",
    "print('val_f1_score ',tss['xgBoost']['val_scores'])\n",
    "print('train_f1_score ',tss['xgBoost']['train_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    }
   ],
   "source": [
    "n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "objective = tss['xgBoost']['specs'][0]['objective']\n",
    "booster = tss['xgBoost']['specs'][0]['booster']\n",
    "gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "print('                 ')\n",
    "print('Re-Training again for calculating f1 and train scores')\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "         max_depth=max_depth, learning_rate=learning_rate, \n",
    "         objective=objective, booster=booster,n_jobs=-1, \n",
    "         gamma=gamma, min_child_weight=min_child_weight, \n",
    "         max_delta_step=max_delta_step, subsample=subsample, \n",
    "         colsample_bytree=colsample_bytree, \n",
    "         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tss['xgBoost']['specs'][0]['feature_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2011 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2011_Anuj_Method/Features_100m_quantile@9.csv\"\n",
    "df_2011 = pd.read_csv(file_2011)\n",
    "\n",
    "ground_truth1 = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/District - Ground Truth - 2011.csv\")\n",
    "cols1 = ['census_code','MSW_2011','BF_2011','MSL_2011', 'FC_2011','CHH_2011','ASSET_2011']\n",
    "ground_truth1 = ground_truth1[cols1]\n",
    "data1 = ground_truth1.merge(df_2011, how='left', on='census_code')\n",
    "data1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['census_code', 'MSW_2011', 'BF_2011', 'MSL_2011', 'FC_2011', 'CHH_2011',\n",
       "       'ASSET_2011', 'band_1_1', 'band_1_2', 'band_1_3',\n",
       "       ...\n",
       "       'band_11_9', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4',\n",
       "       'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9'],\n",
       "      dtype='object', length=115)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_2_1',\n",
      "       ...\n",
      "       'band_11_9', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4',\n",
      "       'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9'],\n",
      "      dtype='object', length=108)\n"
     ]
    }
   ],
   "source": [
    "feture_cols1 = data1.columns[7:]\n",
    "print(feture_cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633, 108)\n",
      "(633,)\n"
     ]
    }
   ],
   "source": [
    "X_2011 = data1[feture_cols1].values\n",
    "print(X_2011.shape)\n",
    "y_2011 = data1['CHH_2011'].values\n",
    "print(y_2011.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587, 115)\n",
      "(587, 108) (587,)\n"
     ]
    }
   ],
   "source": [
    "# 2001\n",
    "df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@9.csv\")\n",
    "# For 2001\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "feature_cols = data.columns[7:].tolist()\n",
    "y_col = 'CHH_2001'\n",
    "# output = apply_smote(data, feature_cols, y_col)\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.48912818940975444 0.49289099526066354\n"
     ]
    }
   ],
   "source": [
    "# val_f1score = []\n",
    "# val_accscore = []\n",
    "# train_f1score = []\n",
    "# train_accscore = []\n",
    "f1score = []\n",
    "accscore = []\n",
    "counter=0\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(counter)\n",
    "    counter=counter+1\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    \n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    \n",
    "    best_xgb = xgbc.fit(X_train, y_train)\n",
    "    \n",
    "    prediction_2011 = best_xgb.predict(X_2011)\n",
    "    f1_weight = f1_score(y_2011, prediction_2011, average='weighted')\n",
    "    acc = accuracy_score(y_2011, prediction_2011)\n",
    "\n",
    "    f1score.append(f1_weight)\n",
    "    accscore.append(acc)\n",
    "\n",
    "#     predictions = best_xgb.predict(X_test)\n",
    "#     predictions_train = best_xgb.predict(X_train)\n",
    "\n",
    "#     f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "#     f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "#     acc = accuracy_score(y_test, predictions)\n",
    "#     acc_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "#     val_f1score.append(f1_weight)\n",
    "#     train_f1score.append(f1_weight_train)\n",
    "#     val_accscore.append(acc)\n",
    "#     train_accscore.append(acc_train)\n",
    "\n",
    "# val_f1score = np.array(val_f1score).mean()\n",
    "# train_f1score = np.array(train_f1score).mean()\n",
    "# val_accscore = np.array(val_accscore).mean()\n",
    "# train_accscore = np.array(train_accscore).mean()\n",
    "f1score = np.array(f1score).mean()\n",
    "accscore = np.array(accscore).mean()\n",
    "print(f1score, accscore)\n",
    "# print(val_f1score, train_f1score, val_accscore, train_accscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "      F1 -- Accuracy\n",
    "BF -  0.57 - 0.59  (9)\n",
    "FC -  0.64 - 0.65\n",
    "MSL - 0.56 - 0.54\n",
    "MSW - 0.59 - 0.59\n",
    "ASS - 0.44 - 0.51\n",
    "CHH - 0.48 - 0.49"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2001 trained 2011 predict - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "      F1 -- Accuracy\n",
    "BF -  0.59 - 0.62  (9)\n",
    "FC -  0.64 - 0.63\n",
    "MSL - 0.55 - 0.53\n",
    "MSW - 0.60 - 0.60 (9)\n",
    "ASS - 0.47 - 0.52 (10)\n",
    "CHH - 0.48 - 0.49 (9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2011 trained 2001 predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_smote(data, feature_cols, target):\n",
    "#     '''\n",
    "#     Input:\n",
    "#     data - the original dataframe\n",
    "#     feature_cols - the feature columns (list of columns)\n",
    "#     target - the target column (string value)\n",
    "#     '''\n",
    "#     sm = SMOTE(random_state=42)\n",
    "#     features, targets = sm.fit_resample(data[feature_cols],data[target])\n",
    "#     feature_df = pd.DataFrame(features, columns=feature_cols)\n",
    "#     target_df = pd.DataFrame(targets, columns=[target])\n",
    "#     output = pd.concat([feature_df, target_df], axis=1)\n",
    "#     # Shuffling dataset\n",
    "#     output = output.sample(frac=1).reset_index(drop=True)\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_f1_score  [0.6431964330130807, 0.6376292182829371, 0.6361804097544307, 0.6302816947158044, 0.6171012521632712]\n",
      "train_f1_score  [0.9974629038832928, 0.997474897684118, 0.9974769420885681, 0.9974684131183981, 0.9977780641771448]\n"
     ]
    }
   ],
   "source": [
    "tss = pickle.load( open(\"/Users/arpitjain/Desktop/all_results/all_results/CHH_2011AnujMethod2011_XGB_top_score_specs@13.pkl\", 'rb'))\n",
    "print('val_f1_score ',tss['xgBoost']['val_scores'])\n",
    "print('train_f1_score ',tss['xgBoost']['train_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    }
   ],
   "source": [
    "n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "objective = tss['xgBoost']['specs'][0]['objective']\n",
    "booster = tss['xgBoost']['specs'][0]['booster']\n",
    "gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "print('                 ')\n",
    "print('Re-Training again for calculating f1 and train scores')\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "         max_depth=max_depth, learning_rate=learning_rate, \n",
    "         objective=objective, booster=booster,n_jobs=-1, \n",
    "         gamma=gamma, min_child_weight=min_child_weight, \n",
    "         max_delta_step=max_delta_step, subsample=subsample, \n",
    "         colsample_bytree=colsample_bytree, \n",
    "         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tss['xgBoost']['specs'][0]['feature_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2001 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@13.csv\"\n",
    "df_2001 = pd.read_csv(file_2001)\n",
    "\n",
    "ground_truth1 = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "cols1 = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "ground_truth1 = ground_truth1[cols1]\n",
    "data1 = ground_truth1.merge(df_2001, how='left', on='census_code')\n",
    "data1.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['census_code', 'MSW_2001', 'BF_2001', 'MSL_2001', 'FC_2001', 'CHH_2001',\n",
       "       'ASSET_2001', 'band_1_1', 'band_1_2', 'band_1_3',\n",
       "       ...\n",
       "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
       "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
       "      dtype='object', length=163)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
      "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
      "      dtype='object', length=156)\n"
     ]
    }
   ],
   "source": [
    "feture_cols1 = data1.columns[7:]\n",
    "print(feture_cols1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587, 156)\n",
      "(587,)\n"
     ]
    }
   ],
   "source": [
    "X_2001 = data1[feture_cols1].values\n",
    "print(X_2001.shape)\n",
    "y_2001 = data1['CHH_2001'].values\n",
    "print(y_2001.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633, 163)\n",
      "(633, 156) (633,)\n"
     ]
    }
   ],
   "source": [
    "# 2001\n",
    "df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/2011_Anuj_Method/Features_100m_quantile@13.csv\")\n",
    "# For 2001\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/District - Ground Truth - 2011.csv\")\n",
    "cols = ['census_code','MSW_2011','BF_2011','MSL_2011', 'FC_2011','CHH_2011','ASSET_2011']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "feature_cols = data.columns[7:].tolist()\n",
    "y_col = 'CHH_2011'\n",
    "# output = apply_smote(data, feature_cols, y_col)\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.5011477727669906 0.49301533219761495\n"
     ]
    }
   ],
   "source": [
    "# val_f1score = []\n",
    "# val_accscore = []\n",
    "# train_f1score = []\n",
    "# train_accscore = []\n",
    "f1score = []\n",
    "accscore = []\n",
    "counter=0\n",
    "for train_index, test_index in cv.split(X):\n",
    "    print(counter)\n",
    "    counter=counter+1\n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    best_xgb = xgbc.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    prediction_2001 = best_xgb.predict(X_2001)\n",
    "    f1_weight = f1_score(y_2001, prediction_2001, average='weighted')\n",
    "    acc = accuracy_score(y_2001, prediction_2001)\n",
    "\n",
    "    f1score.append(f1_weight)\n",
    "    accscore.append(acc)\n",
    "\n",
    "#     predictions = best_xgb.predict(X_test)\n",
    "#     predictions_train = best_xgb.predict(X_train)\n",
    "\n",
    "#     f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "#     f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "#     acc = accuracy_score(y_test, predictions)\n",
    "#     acc_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "#     val_f1score.append(f1_weight)\n",
    "#     train_f1score.append(f1_weight_train)\n",
    "#     val_accscore.append(f1_weight)\n",
    "#     train_accscore.append(f1_weight_train)\n",
    "\n",
    "# val_f1score = np.array(val_f1score).mean()\n",
    "# train_f1score = np.array(train_f1score).mean()\n",
    "# val_accscore = np.array(val_accscore).mean()\n",
    "# train_accscore = np.array(train_accscore).mean()\n",
    "f1score = np.array(f1score).mean()\n",
    "accscore = np.array(accscore).mean()\n",
    "print(f1score, accscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "    f1 - acc\n",
    "BF  0.67 - 0.65\n",
    "FC  0.64 - 0.66\n",
    "MSL 0.48 - 0.53\n",
    "MSW 0.53 - 0.57\n",
    "CHH 0.50 - 0.49\n",
    "ASS 0.62 - 0.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2011 trained 2001 predict - end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on subsequent years - Start - using model of 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bins to use\n",
    "\n",
    "BF - 9\n",
    "CHH - 9\n",
    "MSW - 8\n",
    "ASSET - 8\n",
    "FC - 11\n",
    "MSL - 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_f1_score  [0.6474716234374724, 0.6360490752375146, 0.614048502186572, 0.6036207181410378, 0.603385379491421]\n",
      "train_f1_score  [0.9967497584487287, 0.9967963618113813, 0.9967301638630058, 0.9971516025739577, 0.9967517873452992]\n"
     ]
    }
   ],
   "source": [
    "tss = pickle.load( open(\"/Users/arpitjain/Desktop/all_results/all_results/MSL_2001AnujMethod2001_XGB_top_score_specs@10.pkl\", 'rb'))\n",
    "print('val_f1_score ',tss['xgBoost']['val_scores'])\n",
    "print('train_f1_score ',tss['xgBoost']['train_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    }
   ],
   "source": [
    "n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "objective = tss['xgBoost']['specs'][0]['objective']\n",
    "booster = tss['xgBoost']['specs'][0]['booster']\n",
    "gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "print('                 ')\n",
    "print('Re-Training again for calculating f1 and train scores')\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "         max_depth=max_depth, learning_rate=learning_rate, \n",
    "         objective=objective, booster=booster,n_jobs=-1, \n",
    "         gamma=gamma, min_child_weight=min_child_weight, \n",
    "         max_delta_step=max_delta_step, subsample=subsample, \n",
    "         colsample_bytree=colsample_bytree, \n",
    "         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tss['xgBoost']['specs'][0]['feature_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5',\n",
      "       'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'],\n",
      "      dtype='object', length=120)\n",
      "(633, 120)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5',\n",
      "       'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'],\n",
      "      dtype='object', length=120)\n",
      "(634, 120)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5',\n",
      "       'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'],\n",
      "      dtype='object', length=120)\n",
      "(634, 120)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5',\n",
      "       'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'],\n",
      "      dtype='object', length=120)\n",
      "(634, 120)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5',\n",
      "       'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'],\n",
      "      dtype='object', length=120)\n",
      "(635, 120)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5',\n",
      "       'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'],\n",
      "      dtype='object', length=120)\n",
      "(635, 120)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "file_2011 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2011_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df_2011 = pd.read_csv(file_2011)\n",
    "feature_cols_2011 = df_2011.columns[:-1]\n",
    "print(feature_cols_2011)\n",
    "X_2011 = df_2011[feature_cols_2011].values\n",
    "y_2011 = df_2011['census_code'].values\n",
    "print(X_2011.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2003 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2003_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df_2003 = pd.read_csv(file_2003)\n",
    "feature_cols_2003 = df_2003.columns[:-1]\n",
    "print(feature_cols_2003)\n",
    "X_2003 = df_2003[feature_cols_2003].values\n",
    "y_2003 = df_2003['census_code'].values\n",
    "print(X_2003.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2005 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2005_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df_2005 = pd.read_csv(file_2005)\n",
    "feature_cols_2005 = df_2005.columns[:-1]\n",
    "print(feature_cols_2005)\n",
    "X_2005 = df_2005[feature_cols_2005].values\n",
    "y_2005 = df_2005['census_code'].values\n",
    "print(X_2005.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2007 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2007_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df_2007 = pd.read_csv(file_2007)\n",
    "feature_cols_2007 = df_2007.columns[:-1]\n",
    "print(feature_cols_2007)\n",
    "X_2007 = df_2007[feature_cols_2007].values\n",
    "y_2007 = df_2007['census_code'].values\n",
    "print(X_2007.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2009 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2009_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df_2009 = pd.read_csv(file_2009)\n",
    "feature_cols_2009 = df_2009.columns[:-1]\n",
    "print(feature_cols_2009)\n",
    "X_2009 = df_2009[feature_cols_2009].values\n",
    "y_2009 = df_2009['census_code'].values\n",
    "print(X_2009.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2001 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2001_Anuj_Method/Features_100m_quantile@10.csv\"\n",
    "df_2001 = pd.read_csv(file_2001)\n",
    "feature_cols_2001 = df_2001.columns[:-1]\n",
    "print(feature_cols_2001)\n",
    "X_2001 = df_2001[feature_cols_2001].values\n",
    "y_2001 = df_2001['census_code'].values\n",
    "print(X_2001.shape)\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_2011 = data1[feture_cols1].values\n",
    "# print(X_2011.shape)\n",
    "# y_2011 = data1['CHH_2011'].values\n",
    "# print(y_2011.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587, 127)\n",
      "(587, 120) (587,)\n"
     ]
    }
   ],
   "source": [
    "# 2001\n",
    "df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2001_Anuj_Method/Features_100m_quantile@10.csv\")\n",
    "# For 2001\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "feature_cols = data.columns[7:].tolist()\n",
    "y_col = 'MSL_2001'\n",
    "# output = apply_smote(data, feature_cols, y_col)\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X, y = sm.fit_resample(X, y)\n",
    "best_xgb = xgbc.fit(X, y)\n",
    "prediction_2001 = best_xgb.predict(X_2001)\n",
    "prediction_2003 = best_xgb.predict(X_2003)\n",
    "prediction_2005 = best_xgb.predict(X_2005)\n",
    "prediction_2007 = best_xgb.predict(X_2007)\n",
    "prediction_2009 = best_xgb.predict(X_2009)\n",
    "prediction_2011 = best_xgb.predict(X_2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2001 = np.vstack((y_2001, prediction_2001)).transpose()\n",
    "dat2003 = np.vstack((y_2003, prediction_2003)).transpose()\n",
    "dat2005 = np.vstack((y_2005, prediction_2005)).transpose()\n",
    "dat2007 = np.vstack((y_2007, prediction_2007)).transpose()\n",
    "dat2009 = np.vstack((y_2009, prediction_2009)).transpose()\n",
    "dat2011 = np.vstack((y_2011, prediction_2011)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "datdf2001 = pd.DataFrame(dat2001, columns = ['census_code', 'predictions_2001'])\n",
    "datdf2003 = pd.DataFrame(dat2003, columns = ['census_code', 'predictions_2003'])\n",
    "datdf2005 = pd.DataFrame(dat2005, columns = ['census_code', 'predictions_2005'])\n",
    "datdf2007 = pd.DataFrame(dat2007, columns = ['census_code', 'predictions_2007'])\n",
    "datdf2009 = pd.DataFrame(dat2009, columns = ['census_code', 'predictions_2009'])\n",
    "datdf2011 = pd.DataFrame(dat2011, columns = ['census_code', 'predictions_2011'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth2 = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "\n",
    "# cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "# ground_truth2 = ground_truth2[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(628, 3)\n",
      "(621, 4)\n",
      "(621, 5)\n",
      "(623, 6)\n",
      "(623, 7)\n",
      "(623, 7)\n"
     ]
    }
   ],
   "source": [
    "findata = datdf2001.merge(datdf2003, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2005, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2007, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2009, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2011, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "findata.dropna(inplace=True)\n",
    "print(findata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "findata.to_csv('ChangeClassifier/input_data/MSL_CC.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_code</th>\n",
       "      <th>predictions_2001</th>\n",
       "      <th>predictions_2003</th>\n",
       "      <th>predictions_2005</th>\n",
       "      <th>predictions_2007</th>\n",
       "      <th>predictions_2009</th>\n",
       "      <th>predictions_2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>339</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>469</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   census_code  predictions_2001  predictions_2003  predictions_2005  \\\n",
       "0           80                 3                 2                 1   \n",
       "1          178                 1                 1                 1   \n",
       "2          339                 1                 3                 3   \n",
       "3          469                 2                 2                 2   \n",
       "4          352                 1                 1                 1   \n",
       "\n",
       "   predictions_2007  predictions_2009  predictions_2011  \n",
       "0                 1                 1                 3  \n",
       "1                 1                 1                 1  \n",
       "2                 2                 2                 3  \n",
       "3                 2                 2                 2  \n",
       "4                 1                 1                 1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findata = ground_truth.merge(findata, on='census_code', how='left')\n",
    "# findata.dropna(inplace=True)\n",
    "# print(findata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on subsequent years - Start - using model of 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bins to use\n",
    "\n",
    "BF - 7\n",
    "FC - 7\n",
    "CHH - 13\n",
    "MSW - 13\n",
    "ASSET - 13\n",
    "MSL - 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_f1_score  [0.7228109895456692, 0.7018259387002863, 0.7012544165530012, 0.6945745655118856, 0.6832259554032072]\n",
      "train_f1_score  [0.9982776242145605, 0.9980354222839811, 0.9980268428743304, 0.9980397173128631, 0.9980388577590318]\n"
     ]
    }
   ],
   "source": [
    "tss = pickle.load( open(\"/Users/arpitjain/Desktop/all_results/all_results/MSL_2011AnujMethod2011_XGB_top_score_specs@13.pkl\", 'rb'))\n",
    "print('val_f1_score ',tss['xgBoost']['val_scores'])\n",
    "print('train_f1_score ',tss['xgBoost']['train_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    }
   ],
   "source": [
    "n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "objective = tss['xgBoost']['specs'][0]['objective']\n",
    "booster = tss['xgBoost']['specs'][0]['booster']\n",
    "gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "\n",
    "print('                 ')\n",
    "print('Re-Training again for calculating f1 and train scores')\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "         max_depth=max_depth, learning_rate=learning_rate, \n",
    "         objective=objective, booster=booster,n_jobs=-1, \n",
    "         gamma=gamma, min_child_weight=min_child_weight, \n",
    "         max_delta_step=max_delta_step, subsample=subsample, \n",
    "         colsample_bytree=colsample_bytree, \n",
    "         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "cv = KFold(n_splits=n_splits, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tss['xgBoost']['specs'][0]['feature_cols'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_2011 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2011_Anuj_Method/Features_100m_quantile@7.csv\"\n",
    "# df_2011 = pd.read_csv(file_2011)\n",
    "# feature_cols_2011 = df_2011.columns[:-1]\n",
    "# print(feature_cols_2011)\n",
    "# X_2011 = df_2011[feature_cols_2011].values\n",
    "# y_2011 = df_2011['census_code'].values\n",
    "# print(X_2011.shape)\n",
    "# print('------------')\n",
    "\n",
    "# file_2003 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2003_Anuj_Method/Features_100m_quantile@7.csv\"\n",
    "# df_2003 = pd.read_csv(file_2003)\n",
    "# feature_cols_2003 = df_2003.columns[:-1]\n",
    "# print(feature_cols_2003)\n",
    "# X_2003 = df_2003[feature_cols_2003].values\n",
    "# y_2003 = df_2003['census_code'].values\n",
    "# print(X_2003.shape)\n",
    "# print('------------')\n",
    "\n",
    "# file_2005 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2005_Anuj_Method/Features_100m_quantile@7.csv\"\n",
    "# df_2005 = pd.read_csv(file_2005)\n",
    "# feature_cols_2005 = df_2005.columns[:-1]\n",
    "# print(feature_cols_2005)\n",
    "# X_2005 = df_2005[feature_cols_2005].values\n",
    "# y_2005 = df_2005['census_code'].values\n",
    "# print(X_2005.shape)\n",
    "# print('------------')\n",
    "\n",
    "# file_2007 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2007_Anuj_Method/Features_100m_quantile@7.csv\"\n",
    "# df_2007 = pd.read_csv(file_2007)\n",
    "# feature_cols_2007 = df_2007.columns[:-1]\n",
    "# print(feature_cols_2007)\n",
    "# X_2007 = df_2007[feature_cols_2007].values\n",
    "# y_2007 = df_2007['census_code'].values\n",
    "# print(X_2007.shape)\n",
    "# print('------------')\n",
    "\n",
    "# file_2009 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2009_Anuj_Method/Features_100m_quantile@7.csv\"\n",
    "# df_2009 = pd.read_csv(file_2009)\n",
    "# feature_cols_2009 = df_2009.columns[:-1]\n",
    "# print(feature_cols_2009)\n",
    "# X_2009 = df_2009[feature_cols_2009].values\n",
    "# y_2009 = df_2009['census_code'].values\n",
    "# print(X_2009.shape)\n",
    "# print('------------')\n",
    "\n",
    "# file_2001 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2001_Anuj_Method/Features_100m_quantile@7.csv\"\n",
    "# df_2001 = pd.read_csv(file_2001)\n",
    "# feature_cols_2001 = df_2001.columns[:-1]\n",
    "# print(feature_cols_2001)\n",
    "# X_2001 = df_2001[feature_cols_2001].values\n",
    "# y_2001 = df_2001['census_code'].values\n",
    "# print(X_2001.shape)\n",
    "# print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
      "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
      "      dtype='object', length=156)\n",
      "(633, 156)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
      "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
      "      dtype='object', length=156)\n",
      "(641, 156)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
      "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
      "      dtype='object', length=156)\n",
      "(641, 156)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
      "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
      "      dtype='object', length=156)\n",
      "(634, 156)\n",
      "------------\n",
      "Index(['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6',\n",
      "       'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10',\n",
      "       ...\n",
      "       'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8',\n",
      "       'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'],\n",
      "      dtype='object', length=156)\n",
      "(641, 156)\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "file_2011 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2011_Anuj_Method/Features_100m_quantile@13.csv\"\n",
    "df_2011 = pd.read_csv(file_2011)\n",
    "feature_cols_2011 = df_2011.columns[:-1]\n",
    "print(feature_cols_2011)\n",
    "X_2011 = df_2011[feature_cols_2011].values\n",
    "y_2011 = df_2011['census_code'].values\n",
    "print(X_2011.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2013 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2013_Anuj_Method/Pha_2013_Features_100m_quantile@13.csv\"\n",
    "df_2013 = pd.read_csv(file_2013)\n",
    "feature_cols_2013 = df_2013.columns[:-1]\n",
    "print(feature_cols_2013)\n",
    "X_2013 = df_2013[feature_cols_2013].values\n",
    "y_2013 = df_2013['census_code'].values\n",
    "print(X_2013.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2015 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2015_Anuj_Method/Pha_2015_Features_100m_quantile@13.csv\"\n",
    "df_2015 = pd.read_csv(file_2015)\n",
    "feature_cols_2015 = df_2015.columns[:-1]\n",
    "print(feature_cols_2015)\n",
    "X_2015 = df_2015[feature_cols_2015].values\n",
    "y_2015 = df_2015['census_code'].values\n",
    "print(X_2015.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2017 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2017_Anuj_Method/Features_100m_quantile@13.csv\"\n",
    "df_2017 = pd.read_csv(file_2017)\n",
    "feature_cols_2017 = df_2017.columns[:-1]\n",
    "print(feature_cols_2017)\n",
    "X_2017 = df_2017[feature_cols_2017].values\n",
    "y_2017 = df_2017['census_code'].values\n",
    "print(X_2017.shape)\n",
    "print('------------')\n",
    "\n",
    "file_2019 = \"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2019_Anuj_Method/Pha_2019_Features_100m_quantile@13.csv\"\n",
    "df_2019 = pd.read_csv(file_2019)\n",
    "feature_cols_2019 = df_2019.columns[:-1]\n",
    "print(feature_cols_2019)\n",
    "X_2019 = df_2019[feature_cols_2019].values\n",
    "y_2019 = df_2019['census_code'].values\n",
    "print(X_2019.shape)\n",
    "print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_2011 = data1[feture_cols1].values\n",
    "# print(X_2011.shape)\n",
    "# y_2011 = data1['CHH_2011'].values\n",
    "# print(y_2011.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2001\n",
    "# df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2001_Anuj_Method/Features_100m_quantile@7.csv\")\n",
    "# # For 2001\n",
    "# ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "# cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "# ground_truth = ground_truth[cols]\n",
    "# data = ground_truth.merge(df, how='left', on='census_code')\n",
    "# data.dropna(inplace=True)\n",
    "# print(data.shape)\n",
    "\n",
    "# feature_cols = data.columns[7:].tolist()\n",
    "# y_col = 'BF_2001'\n",
    "# # output = apply_smote(data, feature_cols, y_col)\n",
    "# X = data[feature_cols].values\n",
    "# y = data[y_col].values\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633, 163)\n",
      "(633, 156) (633,)\n"
     ]
    }
   ],
   "source": [
    "# 2011\n",
    "df = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/BinsData_AnujMethod/2011_Anuj_Method/Features_100m_quantile@13.csv\")\n",
    "# For 2011\n",
    "ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/2001_L7_data/District - Ground Truth - 2011.csv\")\n",
    "cols = ['census_code','MSW_2011','BF_2011','MSL_2011', 'FC_2011','CHH_2011','ASSET_2011']\n",
    "ground_truth = ground_truth[cols]\n",
    "data = ground_truth.merge(df, how='left', on='census_code')\n",
    "data.dropna(inplace=True)\n",
    "print(data.shape)\n",
    "\n",
    "feature_cols = data.columns[7:].tolist()\n",
    "y_col = 'MSL_2011'\n",
    "# output = apply_smote(data, feature_cols, y_col)\n",
    "X = data[feature_cols].values\n",
    "y = data[y_col].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X, y = sm.fit_resample(X, y)\n",
    "best_xgb = xgbc.fit(X, y)\n",
    "prediction_2011 = best_xgb.predict(X_2011)\n",
    "prediction_2013 = best_xgb.predict(X_2013)\n",
    "prediction_2015 = best_xgb.predict(X_2015)\n",
    "prediction_2017 = best_xgb.predict(X_2017)\n",
    "prediction_2019 = best_xgb.predict(X_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat2011 = np.vstack((y_2011, prediction_2011)).transpose()\n",
    "dat2013 = np.vstack((y_2013, prediction_2013)).transpose()\n",
    "dat2015 = np.vstack((y_2015, prediction_2015)).transpose()\n",
    "dat2017 = np.vstack((y_2017, prediction_2017)).transpose()\n",
    "dat2019 = np.vstack((y_2019, prediction_2019)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datdf2011 = pd.DataFrame(dat2011, columns = ['census_code', 'predictions_2011'])\n",
    "datdf2013 = pd.DataFrame(dat2013, columns = ['census_code', 'predictions_2013'])\n",
    "datdf2015 = pd.DataFrame(dat2015, columns = ['census_code', 'predictions_2015'])\n",
    "datdf2017 = pd.DataFrame(dat2017, columns = ['census_code', 'predictions_2017'])\n",
    "datdf2019 = pd.DataFrame(dat2019, columns = ['census_code', 'predictions_2019'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datdf2011 = pd.DataFrame(dat2001, columns = ['census_code', 'predictions_2001'])\n",
    "# datdf2013 = pd.DataFrame(dat2003, columns = ['census_code', 'predictions_2003'])\n",
    "# datdf2015 = pd.DataFrame(dat2005, columns = ['census_code', 'predictions_2005'])\n",
    "# datdf2017 = pd.DataFrame(dat2007, columns = ['census_code', 'predictions_2007'])\n",
    "# datdf2019 = pd.DataFrame(dat2009, columns = ['census_code', 'predictions_2009'])\n",
    "# datdf2011 = pd.DataFrame(dat2011, columns = ['census_code', 'predictions_2011'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth2 = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "\n",
    "# cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "# ground_truth2 = ground_truth2[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findata = datdf2001.merge(datdf2003, on='census_code', how='inner')\n",
    "# print(findata.shape)\n",
    "\n",
    "# findata = findata.merge(datdf2005, on='census_code', how='inner')\n",
    "# print(findata.shape)\n",
    "\n",
    "# findata = findata.merge(datdf2007, on='census_code', how='inner')\n",
    "# print(findata.shape)\n",
    "\n",
    "# findata = findata.merge(datdf2009, on='census_code', how='inner')\n",
    "# print(findata.shape)\n",
    "\n",
    "# findata = findata.merge(datdf2011, on='census_code', how='inner')\n",
    "# print(findata.shape)\n",
    "# findata.dropna(inplace=True)\n",
    "# print(findata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(633, 3)\n",
      "(633, 4)\n",
      "(633, 5)\n",
      "(633, 6)\n",
      "(633, 6)\n"
     ]
    }
   ],
   "source": [
    "findata = datdf2011.merge(datdf2013, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2015, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2017, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "\n",
    "findata = findata.merge(datdf2019, on='census_code', how='inner')\n",
    "print(findata.shape)\n",
    "findata.dropna(inplace=True)\n",
    "print(findata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "findata.to_csv('ChangeClassifier/input_data_2011-19/MSL_CC.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>census_code</th>\n",
       "      <th>predictions_2011</th>\n",
       "      <th>predictions_2013</th>\n",
       "      <th>predictions_2015</th>\n",
       "      <th>predictions_2017</th>\n",
       "      <th>predictions_2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>339</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>469</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>352</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   census_code  predictions_2011  predictions_2013  predictions_2015  \\\n",
       "0           80                 3                 1                 3   \n",
       "1          178                 1                 1                 1   \n",
       "2          339                 2                 2                 2   \n",
       "3          469                 2                 3                 2   \n",
       "4          352                 1                 3                 1   \n",
       "\n",
       "   predictions_2017  predictions_2019  \n",
       "0                 3                 3  \n",
       "1                 1                 1  \n",
       "2                 2                 2  \n",
       "3                 2                 3  \n",
       "4                 1                 1  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findata = ground_truth.merge(findata, on='census_code', how='left')\n",
    "# findata.dropna(inplace=True)\n",
    "# print(findata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on subsequent years - End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(indicator, year, folder):\n",
    "    print('###   Running for ', indicator)\n",
    "    \n",
    "    dict_ = {}\n",
    "    for file in os.listdir(folder):\n",
    "        if year in file:\n",
    "            if indicator in file:\n",
    "                bincount = file.split('@')[1].split('.')[0]\n",
    "\n",
    "                filepath = return_filepath(year, bincount)\n",
    "                print('     ### Extracting features from ',filepath)\n",
    "                df = pd.read_csv(filepath)\n",
    "                # For 2001\n",
    "                ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "                cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "                ground_truth = ground_truth[cols]\n",
    "                data = ground_truth.merge(df, how='left', on='census_code')\n",
    "                data.dropna(inplace=True)\n",
    "                \n",
    "                feature_cols = data.columns[7:].tolist()\n",
    "                y_col = indicator + '_' + year\n",
    "                output = apply_smote(data, feature_cols, y_col)\n",
    "                X = output[feature_cols].values\n",
    "                y = output[y_col].values\n",
    "                \n",
    "                print('           ',feature_cols,y_col)\n",
    "                print('------->>>',y_col)\n",
    "                print('-------------')\n",
    "                \n",
    "                tss = pickle.load( open(os.path.join(folder, file), 'rb'))\n",
    "                print('Loading model from - ',file)\n",
    "                print('val_f1_score ',tss['xgBoost']['val_scores'])\n",
    "                print('train_f1_score ',tss['xgBoost']['train_scores'])\n",
    "\n",
    "                n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "                max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "                learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "                objective = tss['xgBoost']['specs'][0]['objective']\n",
    "                booster = tss['xgBoost']['specs'][0]['booster']\n",
    "                gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "                min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "                max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "                subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "                colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "                colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "                colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "                reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "                reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "                scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "                base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "\n",
    "                n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "                \n",
    "                print('                 ')\n",
    "                print('Re-Training again for calculating f1 and train scores')\n",
    "\n",
    "                xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "                         max_depth=max_depth, learning_rate=learning_rate, \n",
    "                         objective=objective, booster=booster,n_jobs=-1, \n",
    "                         gamma=gamma, min_child_weight=min_child_weight, \n",
    "                         max_delta_step=max_delta_step, subsample=subsample, \n",
    "                         colsample_bytree=colsample_bytree, \n",
    "                         colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "                         reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                         scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "                cv = KFold(n_splits=n_splits, shuffle=True)\n",
    "\n",
    "                val_f1score = []\n",
    "                val_accscore = []\n",
    "                train_f1score = []\n",
    "                train_accscore = []\n",
    "                for train_index, test_index in cv.split(X):\n",
    "                    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "                    best_xgb = xgbc.fit(X_train, y_train)\n",
    "\n",
    "                    predictions = best_xgb.predict(X_test)\n",
    "                    predictions_train = best_xgb.predict(X_train)\n",
    "\n",
    "                    f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "                    f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "                    acc = accuracy_score(y_test, predictions)\n",
    "                    acc_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "                    val_f1score.append(f1_weight)\n",
    "                    train_f1score.append(f1_weight_train)\n",
    "                    val_accscore.append(f1_weight)\n",
    "                    train_accscore.append(f1_weight_train)\n",
    "\n",
    "                val_f1score = np.array(val_f1score).mean()\n",
    "                train_f1score = np.array(train_f1score).mean()\n",
    "                val_accscore = np.array(val_accscore).mean()\n",
    "                train_accscore = np.array(train_accscore).mean()\n",
    "                \n",
    "                dict_key = str(year) + '@' + str(indicator) + '@' + str(bincount)\n",
    "                dict_[dict_key] = [val_f1score, train_f1score, val_accscore, train_accscore, tss['xgBoost']['val_scores'], tss['xgBoost']['train_scores']]\n",
    "\n",
    "                \n",
    "                print('~~~~~~~~~~~~')\n",
    "                print('~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc')\n",
    "                print(val_f1score, train_f1score, val_accscore, train_accscore)\n",
    "                print('******************************************')\n",
    "    return dict_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###   Running for  BF\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@9.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@9.pkl\n",
      "val_f1_score  [0.9318887422980637, 0.9093654638489923, 0.9036581866541626, 0.9034736620244039, 0.9002418731694748]\n",
      "train_f1_score  [0.9966943723930293, 0.9966946739558862, 0.9966944152750546, 0.9966948992919493, 0.9966946930345998]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9226625644121762 0.9966942145910809 0.9226625644121762 0.9966942145910809\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@8.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@8.pkl\n",
      "val_f1_score  [0.9161515924661392, 0.9086751441739309, 0.9035565532955869, 0.9010171268351634, 0.897601568277351]\n",
      "train_f1_score  [0.9966946566938374, 0.996694626764428, 0.9966947365326876, 0.9966946174284272, 0.9966948552536632]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.901036583983504 0.9966946336604392 0.901036583983504 0.9966946336604392\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@15.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10', 'band_1_11', 'band_1_12', 'band_1_13', 'band_1_14', 'band_1_15', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_2_10', 'band_2_11', 'band_2_12', 'band_2_13', 'band_2_14', 'band_2_15', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_3_10', 'band_3_11', 'band_3_12', 'band_3_13', 'band_3_14', 'band_3_15', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_4_10', 'band_4_11', 'band_4_12', 'band_4_13', 'band_4_14', 'band_4_15', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_5_10', 'band_5_11', 'band_5_12', 'band_5_13', 'band_5_14', 'band_5_15', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_6_10', 'band_6_11', 'band_6_12', 'band_6_13', 'band_6_14', 'band_6_15', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_7_10', 'band_7_11', 'band_7_12', 'band_7_13', 'band_7_14', 'band_7_15', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_8_10', 'band_8_11', 'band_8_12', 'band_8_13', 'band_8_14', 'band_8_15', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_9_10', 'band_9_11', 'band_9_12', 'band_9_13', 'band_9_14', 'band_9_15', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_10_10', 'band_10_11', 'band_10_12', 'band_10_13', 'band_10_14', 'band_10_15', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_11_10', 'band_11_11', 'band_11_12', 'band_11_13', 'band_11_14', 'band_11_15', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13', 'band_12_14', 'band_12_15'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@15.pkl\n",
      "val_f1_score  [0.935117730618624, 0.921114858164177, 0.9145561588059599, 0.9094521163502526, 0.909404846862952]\n",
      "train_f1_score  [0.9966944470781423, 0.9966948339537879, 0.9969008408768806, 0.9966946315834304, 0.9966943951469549]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.920433123542465 0.9966944376331466 0.920433123542465 0.9966944376331466\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@14.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10', 'band_1_11', 'band_1_12', 'band_1_13', 'band_1_14', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_2_10', 'band_2_11', 'band_2_12', 'band_2_13', 'band_2_14', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_3_10', 'band_3_11', 'band_3_12', 'band_3_13', 'band_3_14', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_4_10', 'band_4_11', 'band_4_12', 'band_4_13', 'band_4_14', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_5_10', 'band_5_11', 'band_5_12', 'band_5_13', 'band_5_14', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_6_10', 'band_6_11', 'band_6_12', 'band_6_13', 'band_6_14', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_7_10', 'band_7_11', 'band_7_12', 'band_7_13', 'band_7_14', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_8_10', 'band_8_11', 'band_8_12', 'band_8_13', 'band_8_14', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_9_10', 'band_9_11', 'band_9_12', 'band_9_13', 'band_9_14', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_10_10', 'band_10_11', 'band_10_12', 'band_10_13', 'band_10_14', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_11_10', 'band_11_11', 'band_11_12', 'band_11_13', 'band_11_14', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13', 'band_12_14'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@14.pkl\n",
      "val_f1_score  [0.9326097761718748, 0.9146531899266639, 0.9105109849969211, 0.9095165184840004, 0.8987666541586314]\n",
      "train_f1_score  [0.9966944470781423, 0.9966948339537879, 0.9966946315834304, 0.9969008408768806, 0.9966946986281824]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9277755751434704 0.9966947927422712 0.9277755751434704 0.9966947927422712\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@13.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10', 'band_1_11', 'band_1_12', 'band_1_13', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_2_10', 'band_2_11', 'band_2_12', 'band_2_13', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_3_10', 'band_3_11', 'band_3_12', 'band_3_13', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_4_10', 'band_4_11', 'band_4_12', 'band_4_13', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_5_10', 'band_5_11', 'band_5_12', 'band_5_13', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_6_10', 'band_6_11', 'band_6_12', 'band_6_13', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_7_10', 'band_7_11', 'band_7_12', 'band_7_13', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_8_10', 'band_8_11', 'band_8_12', 'band_8_13', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_9_10', 'band_9_11', 'band_9_12', 'band_9_13', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_10_10', 'band_10_11', 'band_10_12', 'band_10_13', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_11_10', 'band_11_11', 'band_11_12', 'band_11_13', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12', 'band_12_13'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@13.pkl\n",
      "val_f1_score  [0.9194478429752081, 0.9149479716077167, 0.9129434669975268, 0.9112599206784951, 0.9017504982653983]\n",
      "train_f1_score  [0.9966945550607356, 0.9966950981750887, 0.9966946506973049, 0.9966946120927321, 0.9966951606528142]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9144345755619738 0.9966942731641726 0.9144345755619738 0.9966942731641726\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@12.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10', 'band_1_11', 'band_1_12', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_2_10', 'band_2_11', 'band_2_12', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_3_10', 'band_3_11', 'band_3_12', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_4_10', 'band_4_11', 'band_4_12', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_5_10', 'band_5_11', 'band_5_12', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_6_10', 'band_6_11', 'band_6_12', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_7_10', 'band_7_11', 'band_7_12', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_8_10', 'band_8_11', 'band_8_12', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_9_10', 'band_9_11', 'band_9_12', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_10_10', 'band_10_11', 'band_10_12', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_11_10', 'band_11_11', 'band_11_12', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10', 'band_12_11', 'band_12_12'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@12.pkl\n",
      "val_f1_score  [0.91673747128898, 0.9144385003809973, 0.9109300690112618, 0.9078668521951352, 0.8972920214390235]\n",
      "train_f1_score  [0.9966943194756593, 0.9966943723930293, 0.9966948992919493, 0.9966946739558862, 0.9966944152750546]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9052325180528538 0.9966944726062442 0.9052325180528538 0.9966944726062442\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@10.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_2_10', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_3_10', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_4_10', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_5_10', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_6_10', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_7_10', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_8_10', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_9_10', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_10_10', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_11_10', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@10.pkl\n",
      "val_f1_score  [0.9237348798519331, 0.9159824470448111, 0.9107940249789644, 0.901370945895056, 0.9009686555151287]\n",
      "train_f1_score  [0.9966947910844924, 0.9966950929599123, 0.9966945073024325, 0.9966954604430356, 0.9966947055913428]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9193944908683622 0.9966941885718639 0.9193944908683622 0.9966941885718639\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@11.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_1_8', 'band_1_9', 'band_1_10', 'band_1_11', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_2_8', 'band_2_9', 'band_2_10', 'band_2_11', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_3_8', 'band_3_9', 'band_3_10', 'band_3_11', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_4_8', 'band_4_9', 'band_4_10', 'band_4_11', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_5_8', 'band_5_9', 'band_5_10', 'band_5_11', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_6_8', 'band_6_9', 'band_6_10', 'band_6_11', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_7_8', 'band_7_9', 'band_7_10', 'band_7_11', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_8_8', 'band_8_9', 'band_8_10', 'band_8_11', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_9_8', 'band_9_9', 'band_9_10', 'band_9_11', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_10_8', 'band_10_9', 'band_10_10', 'band_10_11', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_11_8', 'band_11_9', 'band_11_10', 'band_11_11', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7', 'band_12_8', 'band_12_9', 'band_12_10', 'band_12_11'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@11.pkl\n",
      "val_f1_score  [0.9234505006803755, 0.9137315259841345, 0.9096256192109617, 0.8989182044397184, 0.8988641972088542]\n",
      "train_f1_score  [0.9966943723930293, 0.9966946739558862, 0.9966948992919493, 0.9966946930345998, 0.9966944152750546]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9195499302829226 0.996694267546153 0.9195499302829226 0.996694267546153\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@6.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@6.pkl\n",
      "val_f1_score  [0.9300411249481273, 0.9048455693651523, 0.9040553530071236, 0.9027215804057862, 0.9012286068747976]\n",
      "train_f1_score  [0.9966944470781423, 0.9966948339537879, 0.9969008408768806, 0.9966946315834304, 0.9966946986281824]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n",
      "~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~New_Scores : val_f1, train_f1, val_acc, train_acc\n",
      "0.9250796851636116 0.9966948224954102 0.9250796851636116 0.9966948224954102\n",
      "******************************************\n",
      "     ### Extracting features from  /Users/arpitjain/Downloads/SatPRo/2001_L7_data/2001_Anuj_Method/Features_100m_quantile@7.csv\n",
      "            ['band_1_1', 'band_1_2', 'band_1_3', 'band_1_4', 'band_1_5', 'band_1_6', 'band_1_7', 'band_2_1', 'band_2_2', 'band_2_3', 'band_2_4', 'band_2_5', 'band_2_6', 'band_2_7', 'band_3_1', 'band_3_2', 'band_3_3', 'band_3_4', 'band_3_5', 'band_3_6', 'band_3_7', 'band_4_1', 'band_4_2', 'band_4_3', 'band_4_4', 'band_4_5', 'band_4_6', 'band_4_7', 'band_5_1', 'band_5_2', 'band_5_3', 'band_5_4', 'band_5_5', 'band_5_6', 'band_5_7', 'band_6_1', 'band_6_2', 'band_6_3', 'band_6_4', 'band_6_5', 'band_6_6', 'band_6_7', 'band_7_1', 'band_7_2', 'band_7_3', 'band_7_4', 'band_7_5', 'band_7_6', 'band_7_7', 'band_8_1', 'band_8_2', 'band_8_3', 'band_8_4', 'band_8_5', 'band_8_6', 'band_8_7', 'band_9_1', 'band_9_2', 'band_9_3', 'band_9_4', 'band_9_5', 'band_9_6', 'band_9_7', 'band_10_1', 'band_10_2', 'band_10_3', 'band_10_4', 'band_10_5', 'band_10_6', 'band_10_7', 'band_11_1', 'band_11_2', 'band_11_3', 'band_11_4', 'band_11_5', 'band_11_6', 'band_11_7', 'band_12_1', 'band_12_2', 'band_12_3', 'band_12_4', 'band_12_5', 'band_12_6', 'band_12_7'] BF_2001\n",
      "------->>> BF_2001\n",
      "-------------\n",
      "Loading model from -  BF_2001AnujMethod__top_score_specs@7.pkl\n",
      "val_f1_score  [0.9236781962647692, 0.910273321920363, 0.9021490004770811, 0.9010018912370938, 0.8985946857370838]\n",
      "train_f1_score  [0.9966943723930293, 0.996694534686824, 0.9966946739558862, 0.9966944646154486, 0.9966948992919493]\n",
      "                 \n",
      "Re-Training again for calculating f1 and train scores\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1c1fc38e505d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BF'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2001'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-d4b31903f5af>\u001b[0m in \u001b[0;36mget_results\u001b[0;34m(indicator, year, folder)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mbest_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgbc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl_env/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl_env/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl_env/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/automl_env/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bf_dict = get_results('BF','2001',folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 MSW_2001AnujMethod__top_score_specs@10.pkl\n",
      "11 MSW_2001AnujMethod__top_score_specs@11.pkl\n",
      "12 MSW_2001AnujMethod__top_score_specs@12.pkl\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(folder):\n",
    "    \n",
    "    if '2001' in files:\n",
    "        year = 2001\n",
    "        if 'MSW' in files:\n",
    "            indicator ='MSW'\n",
    "            bincount = file.split('@')[1].split('.')[0]\n",
    "            print(bincount, files)\n",
    "            \n",
    "            filepath = return_filepath(year, bincount)\n",
    "            df = pd.read_csv(filepath)\n",
    "            # For 2001\n",
    "            ground_truth = pd.read_csv(\"/Users/arpitjain/Downloads/SatPRo/District - Ground Truth - 2011&2001.csv\")\n",
    "            cols = ['census_code','MSW_2001','BF_2001','MSL_2001', 'FC_2001','CHH_2001','ASSET_2001']\n",
    "            ground_truth = ground_truth[cols]\n",
    "            data = ground_truth.merge(df, how='left', on='census_code')\n",
    "            data.dropna(inplace=True)\n",
    "            \n",
    "            y_col = 'MSW'\n",
    "            output = apply_smote(data, feature_cols, y_col)\n",
    "            X = output[feature_cols].values\n",
    "            y = output[y_col].values\n",
    "            \n",
    "            \n",
    "            \n",
    "            tss = pickle.load( open(os.path.join(folder, file)), 'rb')\n",
    "            print(tss['xgBoost']['val_scores'])\n",
    "            print(tss['xgBoost']['train_scores'])\n",
    "            \n",
    "            n_estimators = tss['xgBoost']['specs'][0]['n_estimators']\n",
    "            max_depth = tss['xgBoost']['specs'][0]['max_depth']\n",
    "            learning_rate = tss['xgBoost']['specs'][0]['learning_rate']\n",
    "            objective = tss['xgBoost']['specs'][0]['objective']\n",
    "            booster = tss['xgBoost']['specs'][0]['booster']\n",
    "            gamma = tss['xgBoost']['specs'][0]['gamma']\n",
    "            min_child_weight = tss['xgBoost']['specs'][0]['min_child_weight']\n",
    "            max_delta_step = tss['xgBoost']['specs'][0]['max_delta_step']\n",
    "            subsample = tss['xgBoost']['specs'][0]['subsample']\n",
    "            colsample_bytree = tss['xgBoost']['specs'][0]['colsample_bytree']\n",
    "            colsample_bylevel = tss['xgBoost']['specs'][0]['colsample_bylevel']\n",
    "            colsample_bynode = tss['xgBoost']['specs'][0]['colsample_bynode']\n",
    "            reg_alpha = tss['xgBoost']['specs'][0]['reg_alpha']\n",
    "            reg_lambda = tss['xgBoost']['specs'][0]['reg_lambda']\n",
    "            scale_pos_weight = tss['xgBoost']['specs'][0]['scale_pos_weight']\n",
    "            base_score = tss['xgBoost']['specs'][0]['base_score']\n",
    "            \n",
    "            n_splits = tss['xgBoost']['specs'][0]['kFold_splits']\n",
    "            \n",
    "            xgbc = XGBClassifier(n_estimators=n_estimators, \n",
    "                     max_depth=max_depth, learning_rate=learning_rate, \n",
    "                     objective=objective, booster=booster,n_jobs=-1, \n",
    "                     gamma=gamma, min_child_weight=min_child_weight, \n",
    "                     max_delta_step=max_delta_step, subsample=subsample, \n",
    "                     colsample_bytree=colsample_bytree, \n",
    "                     colsample_bylevel=colsample_bylevel, colsample_bynode=colsample_bynode, \n",
    "                     reg_alpha=reg_alpha, reg_lambda=reg_lambda, \n",
    "                     scale_pos_weight=scale_pos_weight, base_score=base_score, random_state=0)\n",
    "\n",
    "            cv = KFold(n_splits=n_splits, shuffle=True)\n",
    "    \n",
    "            val_f1score = []\n",
    "            val_accscore = []\n",
    "            train_f1score = []\n",
    "            train_accscore = []\n",
    "            for train_index, test_index in cv.split(X):\n",
    "                X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "                best_xgb = xgbc.fit(X_train, y_train)\n",
    "\n",
    "                predictions = best_xgb.predict(X_test)\n",
    "                predictions_train = best_xgb.predict(X_train)\n",
    "                \n",
    "                f1_weight = f1_score(y_test, predictions, average='weighted')\n",
    "                f1_weight_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "                acc = accuracy_score(y_test, predictions)\n",
    "                acc_train = accuracy_score(y_train, predictions_train)\n",
    "\n",
    "                val_f1score.append(f1_weight)\n",
    "                train_f1score.append(f1_weight_train)\n",
    "                val_accscore.append(f1_weight)\n",
    "                train_accscore.append(f1_weight_train)\n",
    "            \n",
    "            val_f1score = np.array(val_f1score).mean()\n",
    "            train_f1score = np.array(train_f1score).mean()\n",
    "            val_accscore = np.array(val_accscore).mean()\n",
    "            train_accscore = np.array(train_accscore).mean()\n",
    "                \n",
    "                \n",
    "            \n",
    "        elif 'MSL' in files:\n",
    "            indicator ='MSL'\n",
    "        elif 'MSW' in files:\n",
    "            indicator ='MSW'\n",
    "        elif 'MSW' in files:\n",
    "            indicator ='MSW'\n",
    "        elif 'MSW' in files:\n",
    "            indicator ='MSW'\n",
    "        elif 'MSW' in files:\n",
    "            indicator ='MSW'\n",
    "        elif 'MSW' in files:\n",
    "            indicator ='MSW'\n",
    "    \n",
    "    elif '2011' in files:\n",
    "        year =2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
